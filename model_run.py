# -*- coding: utf-8 -*-
"""model_run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xAWmQV8F60MUdlD5yIfRp6tBnZ_F1ifw
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
# from keras.models import load_model
color = sns.color_palette()
sns.set_style('darkgrid')


from joblib import Parallel, delayed
import joblib

data1=pd.read_csv('dataset/data_csv.csv')
data2=pd.read_csv('dataset/Toddler Autism dataset July 2018.csv')
data3=pd.read_csv('dataset/autism_screening.csv')

# data1.head()

# data2.head()

# data3.head()

# print(data1.columns, data1.shape)

# print(data2.columns, data2.shape)
# print(data3.columns, data3.shape)

df1=pd.concat([data1.iloc[:,1:11],data1.iloc[:,[12,22,23,24,25,26,27]]],axis=1)
# print(df1.shape)
# df1.head()

df2=pd.concat([data2.iloc[:,1:12],data2.iloc[:,13:]],axis=1)
df2['Age_Mons']=(df2['Age_Mons']/12).astype(int)
# print(df2.shape)
# df2.head()

df3=pd.concat([data3.iloc[:,0:15],data3.iloc[:,-2:]],axis=1)
# print(df3.shape)
# df3.head()

order_test= pd.DataFrame({
    'df1': df1.columns,
    'df2': df2.columns ,
    'df3': df3.columns
})
# order_test

# Rename columns to have the same names in all DataFrames
df2.columns = df3.columns = df1.columns

# Concatenate the DataFrames
data_fin = pd.concat([df3, df2, df1], axis=0)
# data_fin.head()

# data_fin.shape

# Get object type columns
object_cols = data_fin.select_dtypes('O').columns

# Create new DataFrame
object_df = pd.DataFrame({
    'Objects': object_cols,
    'Unique values': [data_fin[col].unique() for col in object_cols],
    'number of unique values':[data_fin[col].nunique()for col in object_cols]
})

# object_df

# data_fin.columns

# for col in ['Sex', 'Ethnicity',
#        'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test',
#        'ASD_traits']:
#     print("-------------------------------")
#     print(f'Column name: {col}\n')
#     print(f'Unique values:\n{data_fin[col].unique()}')

replacements = {
    'f': 'F',
    'm': 'M',
}
data_fin['Sex'] = data_fin['Sex'].replace(replacements)
replacements = {
    'yes': 'Yes',
    'no': 'No',
}
data_fin['Jaundice'] = data_fin['Jaundice'].replace(replacements)
replacements = {
    'yes': 'Yes',
    'no': 'No',
}
data_fin['Family_mem_with_ASD'] = data_fin['Family_mem_with_ASD'].replace(replacements)
replacements = {
    'YES': 'Yes',
    'NO': 'No',
}
data_fin['ASD_traits'] = data_fin['ASD_traits'].replace(replacements)

replacements = {
    'middle eastern': 'Middle Eastern',
    'Middle Eastern ': 'Middle Eastern',
    'mixed': 'Mixed',
    'asian': 'Asian',
    'black': 'Black',
    'south asian': 'South Asian',
    'PaciFica':'Pacifica',
    'Pasifika':'Pacifica'

}
data_fin['Ethnicity'] = data_fin['Ethnicity'].replace(replacements)

replacements = {
    'Health care professional':'Health Care Professional',
    'family member':'Family Member',
    'Family member':'Family Member'
}
data_fin['Who_completed_the_test'] = data_fin['Who_completed_the_test'].replace(replacements)

# for col in ['Sex', 'Ethnicity',
#        'Jaundice', 'Family_mem_with_ASD', 'Who_completed_the_test',
#        'ASD_traits']:
#     print("-------------------------------")
#     print(f'Column name: {col}\n')
#     print(f'Unique values:\n{data_fin[col].unique()}')

data_fin['Ethnicity'].replace('?', np.nan, inplace=True)
data_fin['Who_completed_the_test'].replace('?', np.nan, inplace=True)
pd.DataFrame(data_fin.isnull().sum(),
             columns=["Missing Values"]).style.bar(color = "#84A9AC")

idf=data_fin.copy()
from sklearn.impute import SimpleImputer

imp = SimpleImputer(strategy='most_frequent')
imputed_data = pd.DataFrame(imp.fit_transform(idf))
imputed_data.columns = idf.columns
imputed_data.index = idf.index

pd.DataFrame(imputed_data.isnull().sum(),
             columns=["Missing Values"]).style.bar(color = "#84A9AC")

# imputed_data.shape

# imputed_data

data = imputed_data.copy()

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

lr = LinearRegression()
dtc = DecisionTreeClassifier()
gclf1 =GaussianNB()
mclf2 = MultinomialNB()
bclf3 =  BernoulliNB()
knn = KNeighborsClassifier()
lgr = LogisticRegression()
rfc = RandomForestClassifier(max_depth = 10, random_state=0)

data = pd.get_dummies(data, columns= ['Ethnicity', 'Who_completed_the_test'], drop_first =  True)
# data.head()

# data.columns

data['Sex'].replace({"M":1, "F":0}, inplace = True)
data['Jaundice'].replace({"Yes":1, "No":0}, inplace = True)
data['Family_mem_with_ASD'].replace({"Yes":1, "No":0}, inplace = True)
# data['ASD_traits'].replace({"Yes":1, "No":0}, inplace = True)
# data.head()
# print(data.columns)

y = data['ASD_traits']
x = data.drop(columns = ['ASD_traits'])

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x, y, random_state=12, test_size=0.2)

dtc.fit(X_train, Y_train)

def Export_DTC():
    return dtc

# # dumpping
# filename = 'dtc_final.pkl'
# pickle.dump(dtc, open(filename, 'wb'))


# # Save the model as a pickle in a file
# joblib.dump(knn, 'dtc_joblib.sav')

y_pred_dtc = dtc.predict(X_test)

# NB
gclf1.fit(X_train, Y_train)
mclf2.fit(X_train, Y_train)
bclf3.fit(X_train, Y_train)

#dumpping
# filename = 'nb_gclf1_final.pkl'
# pickle.dump(gclf1, open(filename, 'wb'))

# filename = 'nb_mclf2_final.pkl'
# pickle.dump(mclf2, open(filename, 'wb'))

# filename = 'nb_bclf3_final.pkl'
# pickle.dump(bclf3, open(filename, 'wb'))

y_pred_gclf1=  gclf1.predict(X_test)
y_pred_mclf2=  mclf2.predict(X_test)
y_pred_bclf3=  bclf3.predict(X_test)

#knn
knn.fit(X_train, Y_train)
y_pred_knn= knn.predict(X_test)

# filename = 'knn_final.pkl'
# pickle.dump(knn, open(filename, 'wb'))

#logistic regression
lgr.fit(X_train,Y_train)

# dumpping
# filename = 'lgr_final.pkl'
# pickle.dump(lgr, open(filename, 'wb'))

Y_pred_lgr = lgr.predict(X_test)

# print("x_test", X_test[:10])
# print("Y_pred_lgr_bharat = ", Y_pred_lgr[:10])
# print("Y_pred_lgr_loaded = ", Y_pred_lgr_loaded[:10])

# Random Forest
rfc.fit(X_train, Y_train)

def Export_RFC():
    return rfc
# filename = 'rfc_final.pkl'
# pickle.dump(rfc, open(filename, 'wb'))

Y_pred_rfc = rfc.predict(X_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

print("Decision Tree Classifier: ",accuracy_score(Y_test, y_pred_dtc))
print("GuassianNB: ",accuracy_score(Y_test, y_pred_gclf1))
print("MultinomialNB: ",accuracy_score(Y_test, y_pred_mclf2))
print("BernoulliNB: ",accuracy_score(Y_test, y_pred_bclf3))
print("KNN Classifier: ",accuracy_score(Y_test, y_pred_knn))
print("Logistic Regression: ",accuracy_score(Y_test, Y_pred_lgr))
# print("logistic Regression pickel = ", accuracy_score(Y_test, Y_pred_lgr_loaded))
print("Random Forest Classifier: ",accuracy_score(Y_test, Y_pred_rfc))
print("Confusion matrix of RFC: "), confusion_matrix(Y_test, Y_pred_rfc)

from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC

classifier3 = SVC(probability=True)
voting_classifier = VotingClassifier(estimators=[
('dtc', dtc),
# ('gclf1', gclf1),
# ('mclf2', mclf2),
# ('bclf3', bclf3),
('knn', knn),
('lgr', lgr),
('rfc', rfc),
('svm', classifier3)
], voting='soft')
voting_classifier.fit(X_train, Y_train)
y_pred = voting_classifier.predict(X_test)

# Calculate the accuracy of the ensemble
accuracy = accuracy_score(Y_test, y_pred)
# print("Ensemble Accuracy:", accuracy)

# #dtc
# dtc = pickle.load(open('dtc_final.pkl', 'rb'))
# Y_pred_dtc = dtc.predict(X_test)
# print(type(X_test))
# print(Y_pred_dtc)

# #nb
# gclf1 = pickle.load(open('nb_gclf1_final.pkl', 'rb'))
# Y_pred_gclf1 = gclf1.predict(X_test)

# mclf2 = pickle.load(open('nb_mclf2_final.pkl', 'rb'))
# Y_pred_mclf2 = mclf2.predict(X_test)

# bclf3 = pickle.load(open('nb_bclf3_final.pkl', 'rb'))
# Y_pred_bclf3 = bclf3.predict(X_test)

# #knn
# knn = pickle.load(open('knn_final.pkl', 'rb'))
# Y_pred_knn = knn.predict(X_test)

# #lgr
# lgr = pickle.load(open('lgr_final.pkl', 'rb'))
# Y_pred_lgr = lgr.predict(X_test)

# # rfc
# rfc = pickle.load(open('rfc_final.pkl', 'rb'))
# Y_pred_rfc = rfc.predict(X_test)

# model accuracy by pickle file.
# print("PICKLE FILE")

# print("Decision Tree Classifier: ",accuracy_score(Y_test, Y_pred_dtc))
# print("GuassianNB: ",accuracy_score(Y_test, Y_pred_gclf1))
# print("MultinomialNB: ",accuracy_score(Y_test, Y_pred_mclf2))
# print("BernoulliNB: ",accuracy_score(Y_test, Y_pred_bclf3))
# print("KNN Classifier: ",accuracy_score(Y_test, Y_pred_knn))
# print("Logistic Regression: ",accuracy_score(Y_test, Y_pred_lgr))
# print("Random Forest Classifier: ",accuracy_score(Y_test, Y_pred_rfc))
# print("Confusion matrix of RFC: "), confusion_matrix(Y_test, Y_pred_rfc)

# !python --version

# !pip install session-info